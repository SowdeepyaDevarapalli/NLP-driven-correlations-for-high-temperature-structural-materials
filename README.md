# SPERT Materials Science Information Extraction

This repository contains the complete implementation of SPERT (Span-based Entity and Relation Transformer) for materials science literature mining. This is the **20-paper best-performing model** achieving 39.1% entity F1 and 18.1% relation F1 scores.

## ğŸ“ Repository Structure

```
spert_materials_clean/
â”œâ”€â”€ spert/                    # Core SPERT framework
â”‚   â”œâ”€â”€ args.py              # Command line argument parsing
â”‚   â”œâ”€â”€ config_reader.py     # Configuration file handling
â”‚   â”œâ”€â”€ spert.py            # Main SPERT entry point
â”‚   â”œâ”€â”€ requirements.txt     # Python dependencies
â”‚   â””â”€â”€ spert/              # Core model implementation
â”‚       â”œâ”€â”€ entities.py      # Entity extraction logic
â”‚       â”œâ”€â”€ models.py        # Neural network architectures
â”‚       â”œâ”€â”€ trainer.py       # Training orchestration
â”‚       â”œâ”€â”€ evaluator.py     # Model evaluation
â”‚       â””â”€â”€ [other core files]
â”œâ”€â”€ configs/                  # Configuration files
â”‚   â”œâ”€â”€ train_config.conf    # Training configuration
â”‚   â”œâ”€â”€ eval_config.conf     # Evaluation configuration
â”‚   â”œâ”€â”€ test_config.conf     # Testing configuration
â”‚   â”œâ”€â”€ types.conf           # Entity/relation type definitions
â”‚   â””â”€â”€ types.json           # Type mappings
â”œâ”€â”€ scripts/                  # Training and utility scripts
â”‚   â”œâ”€â”€ train_cross_validation.py  # Main training script
â”‚   â”œâ”€â”€ create_folds.py      # Data preparation for CV
â”‚   â””â”€â”€ setup_spert.py       # Environment setup
â”œâ”€â”€ data/                     # Training data (5-fold CV structure)
â”‚   â”œâ”€â”€ fold_0/
â”‚   â”œâ”€â”€ fold_1/
â”‚   â”œâ”€â”€ fold_2/
â”‚   â”œâ”€â”€ fold_3/
â”‚   â””â”€â”€ fold_4/
â””â”€â”€ spert.json               # Root configuration
```

## ğŸš€ Quick Start

### 1. Environment Setup
```bash
# Install dependencies
pip install -r spert/requirements.txt

# Setup SPERT environment
python scripts/setup_spert.py
```

### 2. Training
```bash
# Run 5-fold cross-validation training
python scripts/train_cross_validation.py
```

### 3. Data Preparation (if needed)
```bash
# Create new data folds from raw data
python scripts/create_folds.py
```

## ğŸ“Š Model Performance

| Metric | Entity Extraction | Relation Extraction |
|--------|------------------|-------------------|
| **F1 Score** | 39.12% | 18.12% |
| **Precision** | 30.45% | 19.93% |
| **Recall** | 61.91% | 21.12% |
| **Accuracy** | 76.75% | 87.45% |

## ğŸ·ï¸ Supported Entity Types

- **MATERIAL**: Chemical compounds, alloys, materials
- **PROPERTY**: Physical/chemical properties
- **VALUE**: Numerical values
- **UNIT**: Measurement units
- **PROCESS**: Manufacturing/synthesis processes
- **STRUCTURE**: Crystal structures, phases
- **CONDITION**: Temperature, pressure conditions
- And 5 additional specialized types

## ğŸ”— Supported Relation Types

- **HAS_PROPERTY**: Material-property relationships
- **HAS_VALUE**: Property-value relationships  
- **HAS_UNIT**: Value-unit relationships
- **OBSERVED_AT**: Measurement context relationships
- **HAS_STRUCTURE**: Material-structure relationships
- And 12 additional relationship types

## ğŸ’» System Requirements

- **GPU**: Recommended for training (tested on Google Colab T4)
- **RAM**: 8GB+ recommended
- **Python**: 3.7+
- **CUDA**: Compatible GPU drivers if using local GPU

## ğŸ“– Usage Examples

### Training from Scratch
```bash
python spert/spert.py train --config configs/train_config.conf
```

### Evaluation
```bash
python spert/spert.py eval --config configs/eval_config.conf
```

### Prediction on New Data
```bash
python spert/spert.py predict --config configs/test_config.conf
```

## ğŸ”¬ Research Context

This implementation represents the best-performing configuration from experiments comparing:
- **10-paper dataset**: Entity-only extraction (relations fail completely)
- **15-paper dataset**: Minimum viable joint extraction (11.1% relation F1)
- **20-paper dataset**: Optimal performance for available data (18.1% relation F1)

## ğŸ“š Key Findings

1. **Critical Dataset Threshold**: Minimum 15 papers required for relation extraction
2. **Joint Learning Complexity**: Relations require significantly more data than entities
3. **Performance Scaling**: Gradual entity improvement vs. threshold effect for relations
4. **Production Readiness**: Current model suitable for research, needs 50+ papers for production

## ğŸ› ï¸ Configuration

Main configuration files:
- `spert.json`: Root configuration
- `configs/train_config.conf`: Training hyperparameters
- `configs/types.conf`: Entity/relation type definitions

## ğŸ“„ Citation

If you use this code, please cite the original SPERT paper:
```
@inproceedings{eberts-ulges-2020-span,
    title = "Span-based Joint Entity and Relation Extraction with Transformer Pre-training",
    author = "Eberts, Markus and Ulges, Adrian",
    booktitle = "Proceedings of the 24th European Conference on Artificial Intelligence",
    year = "2020"
}
```

## ğŸ“§ Contact

For questions about this implementation or the materials science application, please open an issue in this repository.

---

**Note**: This is the production-ready codebase containing only essential files for training and inference. All experimental analysis files have been excluded for clarity.